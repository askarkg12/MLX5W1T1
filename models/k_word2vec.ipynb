{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading (from local zip) and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText8:\n",
    "    def __init__(self, min_count=5):\n",
    "        self.min_count = min_count  # Minimum word frequency for vocabulary inclusion\n",
    "        self.vocab = None  # To store word to index mapping\n",
    "        self.word_counts = None  # To store word frequencies\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        # Load text8 dataset from the local file\n",
    "        with zipfile.ZipFile(filepath) as f:\n",
    "            text = f.read(f.namelist()[0]).decode(\"utf-8\")\n",
    "        return text\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenize text by removing punctuation and splitting by spaces\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\b[a-z]+\\b\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(tokens)\n",
    "        word_counts = {\n",
    "            word: count\n",
    "            for word, count in word_counts.items()\n",
    "            if count >= self.min_count\n",
    "        }\n",
    "\n",
    "        # Create word to index mapping (vocabulary)\n",
    "        vocab = {word: i for i, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "        vocab[\"<UNK>\"] = 0  # Unknown words get a default index of 0\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = word_counts\n",
    "        return vocab, word_counts\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        # Convert the tokenized words to their corresponding indices from the vocab\n",
    "        indices = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in tokens]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words (as tokens): ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "Vocabulary size: 4126\n",
      "First 10 indices of text: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate preprocessing class and execute loading/preprocessing\n",
    "preprocessor = PreprocessText8(min_count=5)\n",
    "\n",
    "# Step 1: Load dataset from the local file\n",
    "text8_data = preprocessor.load_dataset(\n",
    "    \"data/text8-1mb.zip\"\n",
    ")  # Provide the correct file path\n",
    "\n",
    "# Step 2: Preprocess the dataset (tokenization)\n",
    "tokens = preprocessor.preprocess_text(text8_data)\n",
    "\n",
    "# Step 3: Build the vocabulary\n",
    "vocab, word_counts = preprocessor.build_vocab(tokens)\n",
    "\n",
    "# Step 4: Convert the text into indices based on the vocabulary\n",
    "text_indices = preprocessor.text_to_indices(tokens)\n",
    "\n",
    "# Check results\n",
    "print(\"First 10 words (as tokens):\", tokens[:10])\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"First 10 indices of text:\", text_indices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating training data\n",
    "\n",
    "In the skip-gram model, for each word (target word), you try to predict the surrounding words (context words) within a window. For example, if the window size is 2, for the sentence [\"I\", \"love\", \"machine\", \"learning\"], the training pairs would be:\n",
    "\n",
    "(\"love\", \"I\"), (\"love\", \"machine\")\n",
    "(\"machine\", \"love\"), (\"machine\", \"learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataGenerator:\n",
    "    def __init__(self, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size  # Context window size\n",
    "\n",
    "    def generate_training_pairs(self, text_indices):\n",
    "        \"\"\"\n",
    "        Generates (input, context) pairs using the skip-gram model.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for i, target_word in enumerate(text_indices):\n",
    "            # Define the context window range\n",
    "            start = max(i - self.window_size, 0)\n",
    "            end = min(i + self.window_size + 1, len(text_indices))\n",
    "\n",
    "            # For each word in the window (except the target word), generate a pair\n",
    "            for context_word in text_indices[start:i] + text_indices[i + 1 : end]:\n",
    "                pairs.append((target_word, context_word))\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 training pairs (input, context): [(1, 2), (1, 3), (2, 1), (2, 3), (2, 4)]\n",
      "Total training pairs generated: 702390\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SkipGramDataGenerator\n",
    "window_size = 2  # You can adjust the window size\n",
    "data_generator = SkipGramDataGenerator(vocab, window_size)\n",
    "\n",
    "# Step 2: Generate training pairs\n",
    "training_pairs = data_generator.generate_training_pairs(text_indices)\n",
    "\n",
    "# Check the first 5 pairs\n",
    "print(\"First 5 training pairs (input, context):\", training_pairs[:5])\n",
    "print(\"Total training pairs generated:\", len(training_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Building skip-gram model\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "- Input: One-hot vector of size equal to the vocabulary size.\n",
    "\n",
    "- Hidden Layer: Produces the embedding vector (size embedding_dim).\n",
    "\n",
    "- Output: Softmax over the vocabulary size to predict the context word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding Layer: Converts input words (as indices) to dense vectors of size embedding_dim. These vectors represent the word embeddings that will be learned.\n",
    "Linear Layer: Maps the embedding vector to a vector of size vocab_size. This represents the probabilities of each word in the vocabulary being a context word.\n",
    "Forward Pass: The forward method defines how input passes through the layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Embedding layer (hidden layer)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Output layer (vocab size for softmax)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_word):\n",
    "        # Pass through the embedding layer\n",
    "        embed = self.embeddings(input_word)\n",
    "\n",
    "        # Pass through the linear layer (output layer)\n",
    "        output = self.linear(embed)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (logits): tensor([[ 0.2970,  0.3989, -0.0523,  ..., -0.8852, -0.0619,  0.2866],\n",
      "        [ 0.2224, -0.2155, -0.0630,  ..., -0.1476,  0.9516,  0.6456],\n",
      "        [-0.4301,  0.3467, -0.8593,  ..., -0.4915,  0.3792,  1.1154]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100  # You can adjust this dimensionality\n",
    "\n",
    "# Initialize the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Example of how to run a forward pass with a batch of input words\n",
    "sample_input = torch.tensor([1, 2, 3])  # Sample batch of word indices (input words)\n",
    "output = model(sample_input)\n",
    "\n",
    "print(\"Sample output (logits):\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4126"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training the model\n",
    "\n",
    "- Input preparation: Convert the training pairs (input word, context word) into tensors.\n",
    "- Forward pass: For each input word, predict the probability distribution over the context words.\n",
    "- Loss calculation: Use cross-entropy loss to measure how far the predicted probabilities are from the true context word.\n",
    "- Backpropagation: Compute gradients and update model weights using an optimizer.\n",
    "- Repeat for several epochs: Go through the entire dataset multiple times to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, training_pairs):\n",
    "        self.training_pairs = training_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, context_word = self.training_pairs[idx]\n",
    "        return torch.tensor(input_word), torch.tensor(context_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching\n",
    "batch_size = 64\n",
    "training_dataset = SkipGramDataset(training_pairs)\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5  # Adjust as needed\n",
    "embedding_dim = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for input_words, context_words in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Get predictions\n",
    "        output = model(input_words)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, context_words)\n",
    "\n",
    "        # Backward pass: Compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 6.127859456273038\n"
     ]
    }
   ],
   "source": [
    "# Print epoch loss\n",
    "print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'anarchism': [[-0.7744571  -1.3963333   1.6797013  -0.36338466  0.35906884 -0.56441194\n",
      "   1.636218   -0.7358431  -1.329026   -0.35654166  0.7031084   0.5561964\n",
      "  -0.24942185  0.9599037  -0.20940821 -1.0948676  -1.4011997   1.8756366\n",
      "   0.00571415 -0.8789985   0.77912277  0.16245946 -1.2401024   0.84306574\n",
      "  -0.8503677   1.2378135  -0.01179308  0.35991243 -1.3328391  -1.548498\n",
      "  -0.9834696  -0.75525534 -0.5796744   0.15449347  0.42098314  1.4635736\n",
      "  -0.4123435   0.6639243   1.0369222   0.4815263  -1.5264597  -1.7223165\n",
      "  -0.60674995  1.6102108   0.48401964  0.05120361 -0.79321295 -1.6948316\n",
      "   1.1633942  -0.10054369 -0.118899   -0.05108259  0.5949204  -0.91717595\n",
      "   1.3532596  -0.2873556   1.0850874   0.86395913 -0.44619206 -0.78221434\n",
      "   0.43233    -0.4406806   0.6004827   0.79305243 -0.88724846  0.72463626\n",
      "   0.09062869 -2.2093837  -0.42560497  1.7481977  -0.21781035  0.34679073\n",
      "   0.9230143  -1.7262104  -0.47373107  0.13895707 -0.24337332 -0.2147037\n",
      "  -0.4275165   1.4923874  -0.38184613 -0.1512778  -0.77308255 -0.83717805\n",
      "   1.32524     0.7142017  -0.66886675 -1.4058559   1.6585976  -0.38322744\n",
      "   0.27402475 -1.761183   -0.97417086  0.27695066 -0.31973866 -0.13160366\n",
      "  -1.2428973   1.2897952  -1.8549557  -0.2897908 ]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract the embeddings from the trained model\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Extract embeddings from the model\n",
    "def get_word_embedding(word, model, vocab):\n",
    "    # Get the index of the word\n",
    "    idx = vocab.get(word, vocab[\"<UNK>\"])\n",
    "    # Extract the embedding for the word\n",
    "    embedding = model.embeddings(torch.tensor([idx]))\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "\n",
    "# Example: Get the embedding for the word \"anarchism\"\n",
    "word = \"anarchism\"\n",
    "embedding = get_word_embedding(word, model, vocab)\n",
    "print(f\"Embedding for '{word}': {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'anarchism': [('anarchism', np.float32(1.0000001)), ('tournaments', np.float32(0.33770555)), ('private', np.float32(0.30305058)), ('metals', np.float32(0.30206954)), ('honor', np.float32(0.30183873))]\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute the cosine similarity between the word embeddings\n",
    "\n",
    "\n",
    "def get_word_embedding(word, model, vocab):\n",
    "    # Get the index of the word\n",
    "    idx = vocab.get(word, vocab[\"<UNK>\"])\n",
    "    # Extract the embedding for the word and flatten it to 1D\n",
    "    embedding = model.embeddings(torch.tensor([idx])).squeeze()\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Ensure both vectors are 1D arrays\n",
    "    vec1 = vec1.flatten()\n",
    "    vec2 = vec2.flatten()\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "def find_similar_words(word, model, vocab, top_n=5):\n",
    "    word_embedding = get_word_embedding(word, model, vocab)\n",
    "    similarities = {}\n",
    "\n",
    "    for other_word in vocab:\n",
    "        other_embedding = get_word_embedding(other_word, model, vocab)\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities[other_word] = similarity\n",
    "\n",
    "    # Sort by similarity and return the top_n words\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[\n",
    "        :top_n\n",
    "    ]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'anarchism': [('anarchism', np.float32(1.0000001)), ('tournaments', np.float32(0.33770555)), ('private', np.float32(0.30305058)), ('metals', np.float32(0.30206954)), ('honor', np.float32(0.30183873))]\n"
     ]
    }
   ],
   "source": [
    "# Example: Find words most similar to \"anarchism\"\n",
    "similar_words = find_similar_words(\"anarchism\", model, vocab)\n",
    "print(f\"Words most similar to 'anarchism': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"models/saved/skipgram_word2vec_model.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps\n",
    "# train on whole dataset\n",
    "# evaluate results\n",
    "# fine tune on titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
