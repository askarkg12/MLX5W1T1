{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "from collections import Counter\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data loading (from local zip) and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessText8:\n",
    "    def __init__(self, min_count=5, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Initializes the PreprocessText8 class.\n",
    "        Args:\n",
    "        - min_count: Minimum word frequency for vocabulary inclusion.\n",
    "        - batch_size: Number of rows to process in each batch.\n",
    "        \"\"\"\n",
    "        self.min_count = min_count  # Minimum word frequency for vocabulary inclusion\n",
    "        self.batch_size = batch_size  # Batch size for processing data\n",
    "        self.vocab = None  # To store word-to-index mapping\n",
    "        self.word_counts = None  # To store word frequencies\n",
    "\n",
    "    def load_dataset(self, zip_filepath, parquet_filename, text_column):\n",
    "        \"\"\"\n",
    "        Generator to load the dataset from a zipped Parquet file in batches.\n",
    "        Args:\n",
    "        - zip_filepath: Path to the ZIP file containing Parquet files.\n",
    "        - parquet_filename: The specific Parquet file within the ZIP archive.\n",
    "        - text_column: The column in the Parquet file containing the text data.\n",
    "        \"\"\"\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as z:\n",
    "            with z.open(parquet_filename) as f:\n",
    "                # Load the Parquet file into a pandas DataFrame in memory\n",
    "                df = pd.read_parquet(BytesIO(f.read()), engine=\"pyarrow\")\n",
    "\n",
    "                # Process the data in batches\n",
    "                df_iterator = df[text_column].astype(str).values\n",
    "                for i in range(0, len(df_iterator), self.batch_size):\n",
    "                    yield \" \".join(df_iterator[i : i + self.batch_size])\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text by removing punctuation and splitting by spaces.\n",
    "        Args:\n",
    "        - text: Raw text data.\n",
    "        Returns:\n",
    "        - tokens: List of tokens.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\b[a-z]+\\b\", text)\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, tokens):\n",
    "        \"\"\"\n",
    "        Build the vocabulary by counting word frequencies.\n",
    "        Args:\n",
    "        - tokens: List of tokenized words.\n",
    "        Returns:\n",
    "        - vocab: Word-to-index mapping.\n",
    "        - word_counts: Word frequencies.\n",
    "        \"\"\"\n",
    "        word_counts = Counter(tokens)\n",
    "        word_counts = {\n",
    "            word: count\n",
    "            for word, count in word_counts.items()\n",
    "            if count >= self.min_count\n",
    "        }\n",
    "\n",
    "        # Create word-to-index mapping (vocabulary)\n",
    "        vocab = {word: i for i, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "        vocab[\"<UNK>\"] = 0  # Unknown words get a default index of 0\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.word_counts = word_counts\n",
    "        return vocab, word_counts\n",
    "\n",
    "    def text_to_indices(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert tokenized words to their corresponding indices from the vocabulary.\n",
    "        Args:\n",
    "        - tokens: List of tokens.\n",
    "        Returns:\n",
    "        - indices: List of indices corresponding to tokens.\n",
    "        \"\"\"\n",
    "        indices = [self.vocab.get(word, self.vocab[\"<UNK>\"]) for word in tokens]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataGenerator:\n",
    "    def __init__(self, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size  # Context window size\n",
    "\n",
    "    def generate_training_pairs(self, text_indices_batch):\n",
    "        \"\"\"\n",
    "        Generates (input, context) pairs for a batch using the skip-gram model.\n",
    "        Args:\n",
    "        - text_indices_batch: List of word indices (batch of text).\n",
    "        Returns:\n",
    "        - pairs: List of (input_word, context_word) pairs.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for i, target_word in enumerate(text_indices_batch):\n",
    "            # Define the context window range\n",
    "            start = max(i - self.window_size, 0)\n",
    "            end = min(i + self.window_size + 1, len(text_indices_batch))\n",
    "\n",
    "            # For each word in the window (except the target word), generate a pair\n",
    "            for context_word in (\n",
    "                text_indices_batch[start:i] + text_indices_batch[i + 1 : end]\n",
    "            ):\n",
    "                pairs.append((target_word, context_word))\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified process_dataset to handle large datasets in batches\n",
    "def process_dataset_in_batches(\n",
    "    zip_filepath, parquet_filename, text_column, preprocessor\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized function to load, preprocess, and process a dataset in batches.\n",
    "    Args:\n",
    "    - zip_filepath: Path to the ZIP file containing the Parquet file.\n",
    "    - parquet_filename: Name of the Parquet file inside the ZIP archive.\n",
    "    - text_column: The column in the Parquet file containing the text data.\n",
    "    - preprocessor: PreprocessText8 class instance to handle preprocessing.\n",
    "\n",
    "    Yields:\n",
    "    - processed_tokens: List of processed tokens for each batch.\n",
    "    \"\"\"\n",
    "    data_generator = preprocessor.load_dataset(\n",
    "        zip_filepath, parquet_filename, text_column\n",
    "    )\n",
    "    for batch in data_generator:\n",
    "        tokens = preprocessor.preprocess_text(batch)\n",
    "        yield tokens  # Yield tokens batch by batch for processing\n",
    "\n",
    "\n",
    "# Step 1: Process and build vocabulary incrementally\n",
    "def build_vocab_in_batches(preprocessor, zip_filepath, parquet_filename, text_column):\n",
    "    vocab, word_counts = None, None\n",
    "    data_generator = process_dataset_in_batches(\n",
    "        zip_filepath, parquet_filename, text_column, preprocessor\n",
    "    )\n",
    "\n",
    "    # Incrementally build vocab in batches\n",
    "    for token_batch in data_generator:\n",
    "        vocab, word_counts = preprocessor.build_vocab(token_batch)\n",
    "\n",
    "    return vocab, word_counts\n",
    "\n",
    "\n",
    "# Step 2: Convert text batches to indices in batches\n",
    "def convert_to_indices_in_batches(\n",
    "    preprocessor, zip_filepath, parquet_filename, text_column\n",
    "):\n",
    "    indices_generator = []\n",
    "    data_generator = process_dataset_in_batches(\n",
    "        zip_filepath, parquet_filename, text_column, preprocessor\n",
    "    )\n",
    "\n",
    "    # Convert each batch of tokens to indices\n",
    "    for token_batch in data_generator:\n",
    "        indices_batch = preprocessor.text_to_indices(token_batch)\n",
    "        indices_generator.append(indices_batch)\n",
    "\n",
    "    return indices_generator\n",
    "\n",
    "\n",
    "# Step 3: Generate training pairs in batches\n",
    "def generate_training_pairs_in_batches(\n",
    "    data_generator, data_generator_instance, preprocessor, vocab\n",
    "):\n",
    "    training_pairs = []\n",
    "\n",
    "    for text_indices_batch in data_generator_instance:\n",
    "        batch_pairs = data_generator.generate_training_pairs(text_indices_batch)\n",
    "        training_pairs.extend(batch_pairs)\n",
    "\n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating training data\n",
    "\n",
    "In the skip-gram model, for each word (target word), you try to predict the surrounding words (context words) within a window. For example, if the window size is 2, for the sentence [\"I\", \"love\", \"machine\", \"learning\"], the training pairs would be:\n",
    "\n",
    "(\"love\", \"I\"), (\"love\", \"machine\")\n",
    "(\"machine\", \"love\"), (\"machine\", \"learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PreprocessText8 class and SkipGramDataGenerator\n",
    "preprocessor = PreprocessText8(min_count=5, batch_size=1000)\n",
    "window_size = 2\n",
    "data_generator = SkipGramDataGenerator(vocab=None, window_size=window_size)\n",
    "\n",
    "# Step 1: Build vocabulary incrementally in batches\n",
    "vocab, word_counts = build_vocab_in_batches(\n",
    "    preprocessor, \"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\"\n",
    ")\n",
    "\n",
    "# Step 2: Convert the training, test, and validation datasets into word indices in batches\n",
    "train_indices_generator = convert_to_indices_in_batches(\n",
    "    preprocessor, \"data/train.zip\", \"train-00000-of-00001.parquet\", \"text\"\n",
    ")\n",
    "test_indices_generator = convert_to_indices_in_batches(\n",
    "    preprocessor, \"data/test.zip\", \"test-00000-of-00001.parquet\", \"text\"\n",
    ")\n",
    "validate_indices_generator = convert_to_indices_in_batches(\n",
    "    preprocessor, \"data/validation.zip\", \"validation-00000-of-00001.parquet\", \"text\"\n",
    ")\n",
    "\n",
    "# Step 3: Generate training pairs for SkipGram in batches\n",
    "training_pairs = generate_training_pairs_in_batches(\n",
    "    data_generator, train_indices_generator, preprocessor, vocab\n",
    ")\n",
    "test_pairs = generate_training_pairs_in_batches(\n",
    "    data_generator, test_indices_generator, preprocessor, vocab\n",
    ")\n",
    "validation_pairs = generate_training_pairs_in_batches(\n",
    "    data_generator, validate_indices_generator, preprocessor, vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs generated: 61206990\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training pairs generated: {len(training_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Building skip-gram model\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "- Input: One-hot vector of size equal to the vocabulary size.\n",
    "\n",
    "- Hidden Layer: Produces the embedding vector (size embedding_dim).\n",
    "\n",
    "- Output: Softmax over the vocabulary size to predict the context word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Embedding Layer: Converts input words (as indices) to vectors of size embedding_dim. These vectors represent the word embeddings that will be learned.\n",
    "Linear Layer: Maps the embedding vector to a vector of size vocab_size. This represents the probabilities of each word in the vocabulary being a context word.\n",
    "Forward Pass: The forward method defines how input passes through the layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Embedding layer (hidden layer) with vocab_size and embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Linear layer to project the embedding to vocabulary size\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_words):\n",
    "        \"\"\"\n",
    "        Forward pass for batched input words.\n",
    "        input_words: Tensor of shape [batch_size] containing word indices.\n",
    "        \"\"\"\n",
    "        # Convert input words to their corresponding embeddings\n",
    "        embed = self.embeddings(input_words)  # Shape: [batch_size, embedding_dim]\n",
    "\n",
    "        # Pass embeddings through the linear layer to get vocabulary logits\n",
    "        output = self.linear(embed)  # Shape: [batch_size, vocab_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output (logits): tensor([[ 0.1486,  0.2180, -0.3476,  ..., -0.1374, -0.0847,  0.9195],\n",
      "        [-0.5377, -0.0513,  0.1219,  ..., -0.2479,  0.0938, -0.1595],\n",
      "        [-0.7623,  0.4742, -0.1601,  ...,  0.6987,  0.8690,  0.5691]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Logits shape: torch.Size([3, 67428])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "\n",
    "# Initialize the model\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Ensure the model is on CPU (since you're processing locally)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = (\n",
    "    nn.CrossEntropyLoss()\n",
    ")  # Softmax + CrossEntropy for multi-class classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Example of running a forward pass with a batch of input words\n",
    "sample_input = torch.tensor([1, 2, 3], dtype=torch.long, device=device)\n",
    "\n",
    "# Forward pass: Get the logits for the input words\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print out the sample output (logits) and its shape for clarity\n",
    "print(\"Sample output (logits):\", output)\n",
    "print(\"Logits shape:\", output.shape)  # Should be [batch_size, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Training the model\n",
    "\n",
    "- Input preparation: Convert the training pairs (input word, context word) into tensors.\n",
    "- Forward pass: For each input word, predict the probability distribution over the context words.\n",
    "- Loss calculation: Use cross-entropy loss to measure how far the predicted probabilities are from the true context word.\n",
    "- Backpropagation: Compute gradients and update model weights using an optimizer.\n",
    "- Repeat for several epochs: Go through the entire dataset multiple times to improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, training_pairs):\n",
    "        self.training_pairs = training_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, context_word = self.training_pairs[idx]\n",
    "        return torch.tensor(input_word), torch.tensor(context_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGramModel(\n",
       "  (embeddings): Embedding(67428, 50)\n",
       "  (linear): Linear(in_features=50, out_features=67428, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 2\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for training and validation (assuming you have training_pairs and validation_pairs)\n",
    "train_dataset = SkipGramDataset(training_pairs)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dataset = SkipGramDataset(validation_pairs)\n",
    "validate_loader = DataLoader(\n",
    "    validate_dataset, batch_size=batch_size, shuffle=False, num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the validation set\n",
    "def evaluate_model(model, validate_loader, loss_function, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for input_words, context_words in validate_loader:\n",
    "            input_words, context_words = input_words.to(device), context_words.to(\n",
    "                device\n",
    "            )\n",
    "            output = model(input_words)\n",
    "            loss = loss_function(output, context_words)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(validate_loader)  # Return the average validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for input_words, context_words in train_loader:\n",
    "        input_words, context_words = input_words.to(device), context_words.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Get predictions\n",
    "        output = model(input_words)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_function(output, context_words)\n",
    "\n",
    "        # Backward pass: Compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    avg_training_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_training_loss}\")\n",
    "\n",
    "    # Evaluate on the validation dataset after each epoch\n",
    "    avg_validation_loss = evaluate_model(model, validate_loader, loss_function, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_validation_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'anarchism': [[-0.7744571  -1.3963333   1.6797013  -0.36338466  0.35906884 -0.56441194\n",
      "   1.636218   -0.7358431  -1.329026   -0.35654166  0.7031084   0.5561964\n",
      "  -0.24942185  0.9599037  -0.20940821 -1.0948676  -1.4011997   1.8756366\n",
      "   0.00571415 -0.8789985   0.77912277  0.16245946 -1.2401024   0.84306574\n",
      "  -0.8503677   1.2378135  -0.01179308  0.35991243 -1.3328391  -1.548498\n",
      "  -0.9834696  -0.75525534 -0.5796744   0.15449347  0.42098314  1.4635736\n",
      "  -0.4123435   0.6639243   1.0369222   0.4815263  -1.5264597  -1.7223165\n",
      "  -0.60674995  1.6102108   0.48401964  0.05120361 -0.79321295 -1.6948316\n",
      "   1.1633942  -0.10054369 -0.118899   -0.05108259  0.5949204  -0.91717595\n",
      "   1.3532596  -0.2873556   1.0850874   0.86395913 -0.44619206 -0.78221434\n",
      "   0.43233    -0.4406806   0.6004827   0.79305243 -0.88724846  0.72463626\n",
      "   0.09062869 -2.2093837  -0.42560497  1.7481977  -0.21781035  0.34679073\n",
      "   0.9230143  -1.7262104  -0.47373107  0.13895707 -0.24337332 -0.2147037\n",
      "  -0.4275165   1.4923874  -0.38184613 -0.1512778  -0.77308255 -0.83717805\n",
      "   1.32524     0.7142017  -0.66886675 -1.4058559   1.6585976  -0.38322744\n",
      "   0.27402475 -1.761183   -0.97417086  0.27695066 -0.31973866 -0.13160366\n",
      "  -1.2428973   1.2897952  -1.8549557  -0.2897908 ]]\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(word1, word2, model, vocab):\n",
    "    # Get the word indices\n",
    "    idx1 = vocab.get(word1, vocab[\"<UNK>\"])\n",
    "    idx2 = vocab.get(word2, vocab[\"<UNK>\"])\n",
    "\n",
    "    # Get embeddings for both words\n",
    "    embedding1 = model.embeddings(torch.tensor([idx1]))\n",
    "    embedding2 = model.embeddings(torch.tensor([idx2]))\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = F.cosine_similarity(embedding1, embedding2)\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare similarity between \"king\" and \"queen\"\n",
    "similarity = cosine_similarity(\"king\", \"queen\", model, vocab)\n",
    "print(f\"Cosine similarity between 'king' and 'queen': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word analogy test\n",
    "def word_analogy(word1, word2, word3, model, vocab):\n",
    "    \"\"\"\n",
    "    Solves the analogy: word1 is to word2 as word3 is to ?\n",
    "    \"\"\"\n",
    "    # Get word indices\n",
    "    idx1 = vocab.get(word1, vocab[\"<UNK>\"])\n",
    "    idx2 = vocab.get(word2, vocab[\"<UNK>\"])\n",
    "    idx3 = vocab.get(word3, vocab[\"<UNK>\"])\n",
    "\n",
    "    # Get embeddings\n",
    "    embed1 = model.embeddings(torch.tensor([idx1]))\n",
    "    embed2 = model.embeddings(torch.tensor([idx2]))\n",
    "    embed3 = model.embeddings(torch.tensor([idx3]))\n",
    "\n",
    "    # Solve analogy: word1 - word2 + word3\n",
    "    analogy_vector = embed1 - embed2 + embed3\n",
    "\n",
    "    # Find the word closest to analogy_vector\n",
    "    all_embeddings = model.embeddings.weight\n",
    "    similarities = F.cosine_similarity(\n",
    "        analogy_vector, all_embeddings.unsqueeze(0), dim=2\n",
    "    )\n",
    "\n",
    "    # Get the most similar word (excluding input words)\n",
    "    top_match = torch.argmax(similarities, dim=1)\n",
    "    for word, idx in vocab.items():\n",
    "        if idx == top_match:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \"man\" is to \"king\" as \"woman\" is to ?\n",
    "result = word_analogy(\"man\", \"king\", \"woman\", model, vocab)\n",
    "print(f\"'man' is to 'king' as 'woman' is to: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'anarchism': [('anarchism', np.float32(1.0000001)), ('tournaments', np.float32(0.33770555)), ('private', np.float32(0.30305058)), ('metals', np.float32(0.30206954)), ('honor', np.float32(0.30183873))]\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute the cosine similarity between the word embeddings\n",
    "\n",
    "\n",
    "def get_word_embedding(word, model, vocab):\n",
    "    # Get the index of the word\n",
    "    idx = vocab.get(word, vocab[\"<UNK>\"])\n",
    "    # Extract the embedding for the word and flatten it to 1D\n",
    "    embedding = model.embeddings(torch.tensor([idx])).squeeze()\n",
    "    return embedding.detach().numpy()\n",
    "\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Ensure both vectors are 1D arrays\n",
    "    vec1 = vec1.flatten()\n",
    "    vec2 = vec2.flatten()\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "def find_similar_words(word, model, vocab, top_n=5):\n",
    "    word_embedding = get_word_embedding(word, model, vocab)\n",
    "    similarities = {}\n",
    "\n",
    "    for other_word in vocab:\n",
    "        other_embedding = get_word_embedding(other_word, model, vocab)\n",
    "        similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "        similarities[other_word] = similarity\n",
    "\n",
    "    # Sort by similarity and return the top_n words\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[\n",
    "        :top_n\n",
    "    ]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find words most similar to \"anarchism\"\n",
    "similar_words = find_similar_words(\"king\", model, vocab)\n",
    "print(f\"Words most similar to 'king': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"models/saved/skipgram_word2vec_model.pth\"\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps\n",
    "# train on whole dataset\n",
    "# evaluate results\n",
    "# fine tune on titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
